CERN POOL plots!!! plots shuold be corrected for highio-repack slots
-FE pressure on the CERN POOL
-FACTORY view
-JOBS on the T0 schedds
-Fragmentation of the CERN pool
-Pool size by shared/T0 pools
 
SCALE TEST & GLOBAL POOL:
-schedd plots jobs, jobcores, autoclusters, autoclusters_idle, RecentDaemonCoreDutyCycle
------------

==== DONE: =====

1.- Check interaction of requests and resources at the negotiator:

In summary: are we not fully using the global pool because there are no potentially jobs in the queue? Or is it because some scheduling inefficiency?

LastNegotiationCycleCandidateSlots0 = 47391
LastNegotiationCycleCandidateSlots1 = 46559
LastNegotiationCycleCandidateSlots2 = 43776
LastNegotiationCycleCandidateSlots
0, 1 and 2 being the last three iterations

LastNegotiationCycleMatches0 = 2100
LastNegotiationCycleMatches1 = 2186
LastNegotiationCycleMatches2 = 1639

LastNegotiationCycleNumIdleJobs0 = 23778
LastNegotiationCycleNumIdleJobs1 = 25193
LastNegotiationCycleNumIdleJobs2 = 28299

Check: Negotiator classads attributes at   http://research.cs.wisc.edu/htcondor/manual/v8.4/12_Appendix_A.html

For example:

LastNegotiationCycleMatchRate<X>:
Number of matched jobs divided by the duration of this cycle giving jobs per second. The number <X> appended to the attribute name indicates how many negotiation cycles ago this cycle happened.


--------
1b.-

What is the effect of claim worklife? claim worklife is set to:
24 hours for single core
8 hours for multi core

does it prevent us from doing more matching?: yes, the slot is claimed by a schedd, so if the schedd has no more jobs that can be matched, then the resources will be empty! Should we reduce the claim worklife?

--------
DONE: nego time analysed per phase, collection, filterting, sorting and
matchmaking
1c.- Correlate negotiation time not only with number of pending jobs but with number of non-retiring glideins: driven by combinatorics  of both numbers 
 
-------
1d.- Depth-wise filling: DONE?
1e.- active global pool pruning:DONE, removing pilots after a certain time in
the queues
1f.- improved draining: control and tune parameters regulating draining time

in general, better management of pool resources in both glideinWMS and condor stages 

--------
DONE

2.- Remove "fake Held jobs" from the job status monitor view in the case of crab schedds: these are auxiliary jobs that should not be counted, according to Marco. Vanilla vs grid universe?
dagman jobs, scheduler universe?

--------
DONE

3.- Show idle cores for multicore pilots as percentage of total cores in running: why are some resorces systematically better used than others? Example, T1 vs. T2 -> idle is really coming from not having matching requests, not from inefficiencies in the scheduling cycles. 

Can we correlate idle cores per site with idle jobs matching for the site? 
-> Should be easy, as FE already has info on idle jobs per entry

-fraction of idle/total_cores
-fraction of retire/total_idle_cores 

-------

DONE
4.- Calculate and display average values for attributes shown in the global pool status monitor (pilot age, number of jobs executed, etc)


----
DONE

5.- Modify autoclusters queries to show only clusters of pending jobs, not all, as it displays for all, pending and running. Only pending affects nego. time.


----

5b.- DONE Clustering of prod vs crab jobs


----
DONE

6.- Check factory view: running pilots does not match for many sites what the collector tells us. Also, careful factory monitoring will allow us to spot and understand held glideins, which are unnoticed in the collector-derived views.

Keep list of multicore entries updated! => not needed anymore
----


7.- DONE
Add level of resource usage of UCSD pool for things like MIT: they are running in US sites but how much resources are they using??  
Ex: condor_status -pool glidein-collector.t2.ucsd.edu -af SlotType TotalSlotCpus |grep -v Dynamic |sort |uniq -c
about 12k cores??
 

---

8.- DONE

Perhaps I should try to get and plot job info per site and cores by:
 
condor_q -pool cmsgwms-collector-global.cern.ch -global -af JobStatus RequestCpus MATCH_EXP_JOBGLIDEIN_CMSSite |sort |uniq -c
condor_q -pool cmsgwms-collector-global.cern.ch -global -constraint '(JobStatus == 2) && (RequestCPUs == 4) && (MATCH_GLIDEIN_CMSSite == "T2_US_Vanderbilt")'

with workflow information:
condor_q -pool cmsgwms-collector-global.cern.ch -global -constraint '(JobStatus == 2) && (RequestCPUs == 4) && (MATCH_GLIDEIN_CMSSite == "T2_US_Vanderbilt")' -af WMAgent_RequestName |sort |uniq -c

all with WMAgent/CRAB request info:
condor_q -pool cmsgwms-collector-global.cern.ch -global -constraint '(JobStatus == 2)' -af MATCH_GLIDEIN_CMSSite WMAgent_RequestName CRAB_ReqName RequestCPUs |sort |uniq -c


---
DONE
9.- Count number of pilots started and finished in the last delta_t (30 mins? 1 hour?) => not really an issue once we are managing max queued time of pilots.


---


10.- DONE 
Average, min, max values for each time series plot.


--

11.- Monitor and control resource request and utilization of jobs in the global pool: how many unit cells are they requesting? What about usage in running jobs? (memory, disk, cores)

Monitor also CPU time vs walltime*n_cores: over 100% and close to 0%!!

condor_q -pool cmsgwms-collector-global.cern.ch -global -constraint '(JobStatus == 2) && ((RequestMemory>2500*RequestCPUs) || (RequestDisk>20000*RequestCPUs))' -af RequestCPUs RequestMemory MemoryUsage RequestDisk DiskUsage JobStatus|sort |uniq -c

for completed jobs:
condor_history -pool cmsgwms-collector-global.cern.ch -name cmsgwms-submit1.fnal.gov -af 'RemoteUserCpu/(RequestCPUs*RemoteWallClockTime)'|sort -nr |uniq -c
 
---
DONE
12.- Fast benchmark added to the pilots:
https://gitlab.cern.ch/cloud-infrastructure/cloud-benchmark-suite/blob/master/run/fastBmk.py


---
DONE
13.- Validation scripts in James area:

https://github.com/jamesletts/CMSglideinWMSValidation

should be moved to use the SI gitlab area: 

https://gitlab.cern.ch/groups/CMSSI

---

13b.- Monitoring scripts also should  be moved to SI gitlab and/or to aperezca github:
https://github.com/aperezca/CMS-global-pool-monitor

----
DONE
14.- FIX Dashboard CONDOR VIEW FOR MULTICORE JOBS

----
DONE
15.- Separate the HLT VMs (and other like opportunistic?) from the main categories in the jobs and slots of the global pool.
